# Remove all vars
rm(list=ls())
setwd("E:/MIT Data Science and Big Data Analytics/Module 2/Case Study/Predicting Wages")
# Load data
load(file="pay.discrimination.Rdata")
# See variables in the dataset
class(data)
str(data)
# Dimensions of the dataset
dim(data)
# Table of means of each var
stats <- as.matrix(apply(data, 2, mean))
# Load xtable package to create latex tables
library(xtable)
# Assign colname to table stats
colnames(stats) = c("average")
xtable(stats)
# Wage linear regression
fmla1     <-  wage ~ female + sc+ cg+ mw + so + we + exp1 + exp2 + exp3
# Run Linear Specification and compute MSE and R^2
full.fit1 <-  lm(fmla1, data=data)
fit1      <-  summary(full.fit1)
R2.1      <-  fit1$r.squared
R2.adj1   <-  fit1$adj.r.squared
n1        <-  length(fit1$res)
p1        <-  fit1$df[1]
MSE.adj1  <-  (n1/(n1-p1))*mean(fit1$res^2)
# Linear regression: Quadratic specification
fmla2     <- wage ~  female + (sc+ cg+ mw + so + we + exp1 + exp2 + exp3)^2
# Run Quadratic Specification and compute MSE and R^2
full.fit2 <- lm(fmla2, data=data)
fit2      <- summary(full.fit2)
R2.2      <- fit2$r.squared
R2.adj2   <- fit2$adj.r.squared
n2        <- length(fit2$res)
p2        <- fit2$df[1]
MSE.adj2  <- (n2/(n2-p2))*mean(fit2$res^2)
# Summary of linear and quadratic specifications
table1     <- matrix(0, 2, 4)
table1[1,] <- c(p1, R2.1, R2.adj1, MSE.adj1)
table1[2,] <- c(p2, R2.2, R2.adj2, MSE.adj2)
# Print Regresssion Results
colnames(table1) <- c("p", "R^2", "R^2 adj", "MSE adj")
rownames(table1) <- c("basic reg", "flex reg")
# set random number generator
set.seed(123)
# split data into training and test sample
train      <- sample(1:nrow(data), nrow(data)/2)
# run linear specification and compute MSE and R^2 for the test sample
full.fit1  <- lm(fmla1, data=data[train,])
yhat.fit1  <- predict(full.fit1, newdata=data[-train,])
y.test     <- data[-train,]$wage
MSE.fit1   <- summary(lm((y.test-yhat.fit1)^2~1))$coef[1]
R2.fit1    <- 1- MSE.fit1/var(y.test)
# split data into training and test sample
train      <- sample(1:nrow(data), nrow(data)/2)
# run quadratic specification and compute MSE and R^2 for the test sample
full.fit2  <- lm(fmla2, data=data[train,])
yhat.fit2  <- predict(full.fit2, newdata=data[-train,])
y.test     <- data[-train,]$wage
MSE.fit2   <- summary(lm((y.test-yhat.fit2)^2~1))$coef[1]
R2.fit2    <- 1- MSE.fit2/var(y.test)
# Create result table
table2      <- matrix(0, 2, 3)
table2[1,]  <- c(p1, R2.fit1, MSE.fit1)
table2[2,]  <- c(p2, R2.fit2, MSE.fit2)
# Give Columns and Row Names
colnames(table2)  <- c("p ", "R2 test", "MSE test")
rownames(table2)  <- c("basic reg", "flex reg")
# Print Results
print(table1,digits=4)
print(table2,digits=4)
# Clear workspace
rm(list=ls())
# Load xtable library to print table in .text format
library(xtable)
# Load Dataset and see variables and the number of observations.
load(file="pay.discrimination.Rdata")
str(data)
dim(data)
# Attach dataset to current workspace.
attach(data)
# Compute basic stats:
stats.female  <- as.matrix(apply(data[female==1,], 2, mean))
stats.male    <- as.matrix(apply(data[female==0,], 2, mean))
stats         <- cbind(stats.male, stats.female)
# Print basic stats
colnames(stats) = c("male averages", "female averages")
xtable(stats)
# Wage linear regression
fmla1     <- wage ~  female + sc+ cg+ mw + so + we + exp1 + exp2 + exp3
# Run OlS regression, get coefficients, standard errors and 95% confidence interval
full.fit1 <- lm(fmla1, data=data)
est1      <- summary(full.fit1)$coef[2,1:2]
ci1       <-  confint(full.fit1)[2,]
# Linear regression: Quadratic specification
fmla2     <-  wage ~  female + (sc+ cg+ mw + so + we + exp1 + exp2 + exp3)^2
# Run OlS regression, get coefficients, standard errors and 95% confidence interval
full.fit2 <- lm(fmla2, data=data)
est2      <- summary(full.fit2)$coef[2,1:2]
ci2       <- confint(full.fit2)[2,]
#Create table to store regression results
table1     <- matrix(0, 2, 4)
table1[1,] <- c(est1,ci1)
table1[2,] <- c(est2,ci2)
#Give column and  row names
colnames(table1) <- c("Estimate", "Standard Error", "Lower Conf. Bound", "Upper Conf. Bound")
rownames(table1) <- c("basic reg", "flex reg")
# Linear regression of y (outcome) on covariates
fmla1.y <- wage ~  sc+ cg+ mw + so + we + exp1 + exp2 + exp3
# Linear regression of d (treatment) on covariates
fmla1.d <- female ~  sc+ cg+ mw + so + we + exp1 + exp2 + exp3
# Residuals of outcome regression
t.Y    <- lm(fmla1.y, data=data)$res
# Residuals of treatment regression
t.D    <-  lm(fmla1.d, data=data)$res
# Run OLS coefficient get coefficients and 95% confidence intervals
partial.fit1   <- lm(t.Y~t.D)
partial.est1   <- summary(partial.fit1)$coef[2,1:2]
partial.ci1    <- confint(partial.fit1)[2,]
fmla2.y  <- wage ~  (sc+ cg+ mw + so + we + exp1 + exp2 + exp3)^2
fmla2.d  <- female ~ (sc+ cg+ mw + so + we + exp1 + exp2 + exp3)^2
# get residuals from linear regression
t.Y  <- lm(fmla2.y, data=data)$res
t.D  <- lm(fmla2.d, data=data)$res
# regress residuals one onether to get result from partialled out regression
partial.fit2  <-  lm(t.Y~t.D)
partial.est2  <-  summary(partial.fit2)$coef[2,1:2]
partial.ci2   <-  confint(partial.fit2)[2,]
#Create table to store regression results
table2     <- matrix(0, 4, 2)
table2[1,] <- c(est1)
table2[2,] <- c(est2)
table2[3,] <- c(partial.est1)
table2[4,] <- c(partial.est2)
#Give column and row names
colnames(table2) <- c("Estimate", "Standard Error")
rownames(table2) <- c("basic reg", "flex reg", "basic reg with partialling out", "flex reg with partialling out")
#Print results
print(table1, digits=3)
print(table2, digits=3)
summary(full.fit1)
install.packages("hdm")
setwd("E:/MIT Data Science and Big Data Analytics/Module 2/Case Study/Growth of Poor and Rich Countries - Modern Regression Methods")
# Load hdm library
library(hdm)
# Load Dataset and see variables and the number of observations.
load(file="growth.Rdata")
# Load Dataset and see variables and the number of observations.
load(file="growth.Rdata")
dim(growth)
str(growth)
# Get variable names
varnames= colnames(growth)
# Extract the names of control and treatment variables from varnames
xnames     <- varnames[-c(1,2,3)]     # names of X variables
dandxnames <- varnames[-c(1,2)]       # names of D and X variables
# create formulas by pasting names (this saves typing times)
fmla      <-  as.formula(paste("Outcome ~ ", paste(dandxnames, collapse= "+")))
full.fit  <-  lm(fmla, data=growth)
fmla.y    <-  as.formula(paste("Outcome ~ ", paste(xnames, collapse= "+")))
fmla.d    <-  as.formula(paste("gdpsh465~ ", paste(xnames, collapse= "+")))
# partial d and y by linear regression
rY       <- rlasso(fmla.y, data =growth)$res
rD       <- rlasso(fmla.d, data =growth)$res
# regress partialed out Y on partialed out D
partial.fit.lasso <- lm(rY~rD-1)
# create table to store results
table      <- matrix(0, 2, 2)
table[1,]  <- summary(full.fit)$coef["gdpsh465",1:2]
table[2,]  <- summary(partial.fit.lasso)$coef[1,1:2]
# give column and row names
colnames(table) <- names(summary(full.fit)$coef["gdpsh465",])[1:2]
rownames(table) <- c("Least Squares", "Partialling-out via lasso")
# print results
print(table, digits=2)
# Clear workspace
rm(list=ls())
# Load Necessary Libraries
library(hdm)
library(randomForest)
library(glmnet)
library(nnet)
options(warn=-1)
library(rpart)
library(nnet)
library(gbm)
install.packages("gbm")
library(gbm)
library(rpart.plot)
install.packages("rpart.plot")
library(xtable)
library(rpart.plot)
setwd("E:/MIT Data Science and Big Data Analytics/Module 2/Case Study/Predicting Wages Using Several Machine Learning Methods and Data Splitting")
# Load Data
load(file="wage2015.Rdata")
# Split Data into Training and Testing Sample
set.seed(1)
# Set of indices for training
training <- sample(nrow(data), nrow(data)*(1/2), replace=FALSE)
# training data
datause <- data[training,]
# test data
dataout <- data[-training,]
# Linear Control Variables. Use This Control Variables for Tree Based Machine Learning Methods.
x <- "sex+white+black+hisp+shs+hsg+scl+clg+mw+so+we+union+vet+cent+ncent+fam1+fam2+fam3+child+fborn+cit+school+pens+fsize10+fsize100+health+age+exp1+occ2+ind2"
# Quadratic Control Variables(Flexible Specification). Use This Control Variables for Linear methods
xL <- "(sex+white+black+hisp+shs+hsg+scl+clg+mw+so+we+union+vet+cent+ncent+fam1+fam2+fam3+child+fborn+cit+school+pens+fsize10+fsize100+health+age+exp1+exp2+exp3+exp4+occ2+ind2)^2"
# outcome variable log wage
y  <- "lwage"
# Linear Model: Quadratic (Large) Specification
formL <- as.formula(paste(y, "~", xL))
# Linear Model: Linear Specification
form  <- as.formula(paste(y, "~", x))
# "-1" : do not include constant in linear model
# x,y TRUE: return matrix of covariates/vector of outcomes
# Run these linear regression to use their outcome variables (fituse$y) and covariates(fituse$y)
# a trick to extract x and y variables from a formula.
fituseL    <- lm(paste(y, "~", xL, "-1"), datause, x=TRUE, y=TRUE)
fitoutL    <- lm(paste(y, "~", xL, "-1"), dataout, x=TRUE, y=TRUE)
fituse     <- lm(paste(y, "~", x, "-1"), datause, x=TRUE, y=TRUE)
fitout     <- lm(paste(y, "~", x, "-1"), dataout, x=TRUE, y=TRUE)
fituse     <- lm(paste(y, "~", x, "-1"), datause, x=TRUE, y=TRUE)
fitout     <- lm(paste(y, "~", x, "-1"), dataout, x=TRUE, y=TRUE)
# linear regression
fit.lm      <- lm(form, datause)
fit.lm2     <- lm(formL, datause)
#Lasso
fit.rlasso  <- rlasso(form, datause, post=FALSE)
#Post Lasso
fit.rlasso2 <- rlasso(form, datause, post=TRUE)
#CV-Lasso
fit.lasso   <- cv.glmnet(fituse$x, fituse$y, family="gaussian", alpha=1)
#CV-Ridge
fit.ridge   <- cv.glmnet(fituse$x, fituse$y, family="gaussian", alpha=0)
#CV-Elastic Net
fit.elnet   <- cv.glmnet(fituse$x, fituse$y, family="gaussian", alpha=.5)
#Lasso Flexible
fit.rlassoL  <- rlasso(formL, datause, post=FALSE)
#Post-Lasso Flexible
fit.rlasso2L <- rlasso(formL, datause, post=TRUE)
#CV-Lasso Flexible
fit.lassoL   <- cv.glmnet(fituseL$x, fituseL$y, family="gaussian", alpha=1)
setwd("E:/MIT Data Science and Big Data Analytics/Module 2/Case Study/The Effect of Gun Ownership on Homicide Rates")
library(foreign);
library(quantreg);
library(mnormt);
library(gbm);
library(glmnet);
library(MASS);
library(rpart);
library(sandwich);
library(hdm);
library(randomForest);
library(xtable)
library(nnet)
library(neuralnet)
library(caret)
library(matrixStats)
library(devtools)
library(plyr)
# Clear variables.
rm(list = ls())
# Set Random Number Generator.
set.seed(1)
# Load Functions from External Files.
source("Cond-comp.R")
source("Functions.R")
# Load Data
data <- read.csv("gun_clean.csv")
# Table for Storing Results
table <- matrix(0,24,1)
varlist <- function (df=NULL,type=c("numeric","factor","character"), pattern="", exclude=NULL) {
vars <- character(0)
if (any(type %in% "numeric")) {
vars <- c(vars,names(df)[sapply(df,is.numeric)])
}
if (any(type %in% "factor")) {
vars <- c(vars,names(df)[sapply(df,is.factor)])
}
if (any(type %in% "character")) {
vars <- c(vars,names(df)[sapply(df,is.character)])
}
vars[(!vars %in% exclude) & grepl(vars,pattern=pattern)]
}
# Dummy Variables for Year and County Fixed Effects
fixed  <- grep("X_Jfips", names(data), value=TRUE, fixed=TRUE)
year   <- varlist(data, pattern="X_Tyear")
# Census Control Variables
census     <- NULL
census_var <- c("^AGE", "^BN", "^BP", "^BZ", "^ED", "^EL","^HI", "^HS", "^INC", "^LF", "^LN", "^PI", "^PO", "^PP", "^PV", "^SPR", "^VS")
for(i in 1:length(census_var)){
census  <- append(census, varlist(data, pattern=census_var[i]))
}
# Treatment Variable
d     <- "logfssl"
# Outcome Variable
y     <- "logghomr"
# Other Control Variables
X1    <- c("logrobr", "logburg", "burg_missing", "robrate_missing")
X2    <- c("newblack", "newfhh", "newmove", "newdens", "newmal")
# New Dataset for Partiled-out Variables
rdata    <- as.data.frame(data$CountyCode)
colnames(rdata) <- "CountyCode"
# Variables to be Partialled-out
varlist <- c(y, d,X1, X2, census)
# Partial out Variables in varlist from year and county fixed effect
for(i in 1:length(varlist)){
form <- as.formula(paste(varlist[i], "~", paste(paste(year,collapse="+"),  paste(fixed,collapse="+"), sep="+")))
rdata[, varlist[i]] <- lm(form, data)$residuals
}
form1 <- as.formula(paste(y, "~", d ))
form2 <- as.formula(paste(y, "~", paste(d, paste(X1,collapse="+"), paste(X2,collapse="+"), paste(census,collapse="+"),   sep="+")))
table[1:2,1] <- summary(lm(form1, rdata))$coef[2,1:2]
table[3:4,1] <- summary(lm(form2, rdata))$coef[2,1:2]
# Outcome Variable
y.name      <-  y;
# Treatment Indicator
d.name      <- d;
# Controls
x.name      <- paste(paste(X1,collapse="+"),  paste(X2,collapse="+"), paste(census,collapse="+"), sep="+") # use this for tree-based methods like forests and boosted trees
# Method names
method      <- c("RLasso","PostRLasso", "Forest", "Boosting", "Trees", "Lasso", "Ridge", "Elnet", "Nnet")
res <- DoubleML(rdata, y.name, d.name, x.name, x.name, method=method, 2 ,"plinear")
table[5:6,1]   <-res[,1]
table[7:8,1]   <-res[,2]
table[9:10,1]  <-res[,3]
table[11:12,1] <-res[,4]
table[13:14,1] <-res[,5]
table[15:16,1] <-res[,6]
table[17:18,1] <-res[,7]
table[19:20,1] <-res[,8]
table[21:22,1] <-res[,9]
table[23:24,1] <-res[,10]
colnames(table) <- c("Gun")
rownames(table) <- c("Baseline1", "se(Baseline1)", "Baseline2", "sd(Baseline2)", "RLasso","sd(RLasso)", "PostRLasso", "sd(PostRLasso)",  "Forest", "sd(Forest)",
"Boosting", "sd(Boosting)", "Trees", "sd(Trees)", "Lasso", "sd(Lasso)", "Ridge", "sd(Ridge)", "Elnet", "sd(Elnet)", "Nnet", "sd(Nnet)", "Best", "sd(Best)")
print(table, digit=3)
